# Module 1: Labs

These labs are designed to provide hands-on experience with the concepts covered in this module. You should have access to at least two Linux virtual machines (one control plane, one worker) to perform these labs.

---

### Lab 1: Installing a Kubernetes Cluster with `kubeadm`

**Objective:** Create a single-node control plane cluster and join one worker node.

**Instructions:**

1.  **On both nodes (Control Plane and Worker):**
    *   Install a container runtime (like `containerd`).
    *   Install the Kubernetes packages: `kubeadm`, `kubelet`, and `kubectl`.
    *   Disable swap: `sudo swapoff -a`.

2.  **On the Control Plane node only:**
    *   Initialize the cluster using `kubeadm`. Specify a pod network CIDR.
        ```bash
        # Example:
        sudo kubeadm init --pod-network-cidr=192.168.0.0/16
        ```
    *   After the command succeeds, it will print a `kubeadm join` command. **Save this command.**
    *   Follow the instructions to configure `kubectl` for your user:
        ```bash
        mkdir -p $HOME/.kube
        sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
        sudo chown $(id -u):$(id -g) $HOME/.kube/config
        ```

3.  **Install a CNI Plugin on the Control Plane node:**
    *   Choose a CNI provider. Calico is a popular choice.
    *   Apply the Calico manifest to your cluster:
        ```bash
        kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.1/manifests/calico.yaml
        ```
    *   Verify that the control plane node becomes `Ready`:
        ```bash
        kubectl get nodes
        # The status should change from NotReady to Ready after a minute.
        ```

4.  **On the Worker node only:**
    *   Paste and run the `kubeadm join` command you saved from step 2. You will need to run it with `sudo`.
    *   If you lost the token, you can generate a new one on the control plane node: `kubeadm token create --print-join-command`.

5.  **Verification:**
    *   On the control plane node, run `kubectl get nodes`. You should see both your control plane and worker nodes in the `Ready` state.

---

### Lab 2: Managing Cluster Certificates

**Objective:** Understand and manage the certificates generated by `kubeadm`.

**Instructions:**

1.  **On the Control Plane node:**
    *   List all the certificates managed by `kubeadm`:
        ```bash
        sudo kubeadm certs list
        ```
    *   Check the expiration date of the API server certificate:
        ```bash
        sudo kubeadm certs check-expiration
        ```
    *   Manually renew all certificates:
        ```bash
        sudo kubeadm certs renew all
        ```
    *   Verify the new expiration dates.

---

### Lab 3: Backing Up and Restoring `etcd`

**Objective:** Learn how to perform a backup and restore of the `etcd` database, which is critical for disaster recovery.

**Instructions:**

1.  **On the Control Plane node:**
    *   Find the `etcdctl` binary. If it's not installed, you can run it from within the `etcd` static pod.
    *   Identify the necessary arguments for `etcdctl` to connect to the `etcd` server managed by `kubeadm`. You will need the CA certificate, server certificate, and server key. These are located in `/etc/kubernetes/pki/etcd/`.
        ```bash
        # Example of arguments needed for etcdctl
        ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \
          --cacert=/etc/kubernetes/pki/etcd/ca.crt \
          --cert=/etc/kubernetes/pki/etcd/server.crt \
          --key=/etc/kubernetes/pki/etcd/server.key
        ```

2.  **Create a Snapshot:**
    *   Use `etcdctl snapshot save` to create a backup.
        ```bash
        # Using the arguments from the previous step
        ETCDCTL_API=3 etcdctl snapshot save /tmp/etcd-backup.db \
          --endpoints=https://127.0.0.1:2379 \
          --cacert=/etc/kubernetes/pki/etcd/ca.crt \
          --cert=/etc/kubernetes/pki/etcd/server.crt \
          --key=/etc/kubernetes/pki/etcd/server.key
        ```

3.  **Simulate a Disaster:**
    *   Let's create a dummy pod to see if it disappears after the restore.
        ```bash
        kubectl run dummy-pod --image=nginx
        kubectl get pods
        ```
    *   (This is the dangerous part, be careful!) We will simulate data loss by deleting the pod directly from etcd. For a real restore, you would be doing this on a fresh or broken cluster.

4.  **Restore the Snapshot:**
    *   Stop the `kube-apiserver` and `etcd` static pods by moving their manifests out of `/etc/kubernetes/manifests/`.
        ```bash
        sudo mv /etc/kubernetes/manifests/etcd.yaml /tmp/
        sudo mv /etc/kubernetes/manifests/kube-apiserver.yaml /tmp/
        ```
    *   Run the `etcdctl snapshot restore` command. This will create a new data directory.
        ```bash
        ETCDCTL_API=3 etcdctl snapshot restore /tmp/etcd-backup.db \
          --data-dir /var/lib/etcd-new
        ```
    *   Replace the old data directory with the new one: `sudo mv /var/lib/etcd /var/lib/etcd-old && sudo mv /var/lib/etcd-new /var/lib/etcd`.
    *   Restart the control plane pods by moving the manifests back:
        ```bash
        sudo mv /tmp/etcd.yaml /etc/kubernetes/manifests/
        sudo mv /tmp/kube-apiserver.yaml /etc/kubernetes/manifests/
        ```

5.  **Verification:**
    *   Wait a few minutes for the control plane to come back up.
    *   Run `kubectl get pods`. The `dummy-pod` should be gone, as the backup was taken before it was created. This confirms the restore was successful.
